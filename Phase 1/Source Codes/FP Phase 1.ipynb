{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de12d786-ee97-465f-a0ed-6a66e80a540e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## FP Phase 1 - Project Plan, describe datasets, joins, tasks, and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4528a04-9063-493a-b70f-e167950ac709",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Team Members\n",
    "| Name | Email |\n",
    "| :----------- | :------------ | \n",
    "|Zukang Yang| zukangy@ischool.berkeley.edu|\n",
    "|Joey He|joeyhe01@ischool.berkeley.edu|\n",
    "|Sam Meng|smeng@ischool.berkeley.edu|\n",
    "|Nishika Abeytunge|nishika.abey@ischool.berkeley.edu|\n",
    "|Weijie Yang|raphael.yang@ischool.berkeley.edu|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2491cf56-26a8-4d93-af47-ee60ef091acc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Phase Leader Plan\n",
    "\n",
    "| Phase: Title | Leader | Week\n",
    "| :----------- | :------------ | :---------- \n",
    "| Phase 1: Project Plan, describe datasets, joins, tasks, and metrics | Zukang Yang| week 10 \n",
    "| Phase 2: EDA, baseline pipeline on all available data, Scalability, Efficiency, Distributed/parallel Training, Scoring Pipeline, Feature engineering and hyperparameter tuning | Joey He| week 11 \n",
    "| Phase 2: EDA, baseline pipeline on all available data, Scalability, Efficiency, Distributed/parallel Training, Scoring Pipeline, Feature engineering and hyperparameter tuning | Nishika Abeytunge | week 12 \n",
    "| Phase 3: Select the optimal algorithm, fine-tune and submit a final report | Weijie Yang| week 13\n",
    "| Phase 3: Select the optimal algorithm, fine-tune and submit a final report| Sam Meng| week 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "005a9caf-793b-4c3b-b3f7-85400459fd82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Credit Assignment Plan\n",
    "\n",
    "|Phase | Task | Assignee | Effort (in hour)\n",
    "| :------------ | :----------- | :------------: | :------\n",
    "| 1 | Set up Azure storage blob container and link it to Databricks for team usage| Zukang Yang | 1.5h\n",
    "| 1 | Completed phase leader plan | Zukang Yang | 0.2h\n",
    "| 1 | Completed credit assignment plan | Zukang Yang | 1h\n",
    "| 1 | Composed the abstract for phase 1| Sam Meng | 1.5h\n",
    "| 1 | Data Description | Nishika Abeytunge | 2.5h\n",
    "| 1 | Created machine learning pipelines, Gantt chart and timeline | Weijie Yang | 2h\n",
    "| 1 | Designed machine algorithms and metrics | Joey He | 2h\n",
    "| 1 | Edited all members' inputs and proofread the report notebook for submission | Zukang Yang | 2h\n",
    "| 2 | EDA on all tables (raw data) - Ariline on-time performance, weather, weather stations, airport code, and then address missing data & non-numerical features | Sam Meng | 5h\n",
    "| 2 | Think of and list out raw features and derived features (feature engineering); come up with at least one time-based feature | Weijie Yang | 2h\n",
    "| 2 | Dimensionality reduction and why | Joey He | 1h\n",
    "| 2 | list out feature transformations for the pipeline and verify the integrity | Nishika Abeytunge | 2h\n",
    "| 2 | Create ML base pipeline on 3-month ATPW dataset (optional) | Zukang Yang | 5h \n",
    "| 2 | Create ML base pipeline on 1-year ATPW dataset; log and report results| Joey He | 6h\n",
    "| 2 | Fine-tune baseline pipeline | Nishika Abeytunge | 3h\n",
    "| 2 | In-class presentation prep | All members | 2h\n",
    "| 3 | Implement random forest on top on the base pipeline; fine-tune the pipeline if possible| Nishika Abeytunge | 6h\n",
    "| 3 | Implement Gradient Boosted Decision Trees on top on the base pipeline; fine-tune the pipeline if possible | Weijie Yang | 6h\n",
    "| 3 | Implement MLP on top on the base pipeline (extra credit ~ 5pts); fine-tune the pipeline if possible | Zukang Yang | 6h\n",
    "| 3 | Discuss more feature engineering possibilities and update the credit assignment plan | All members | 1h\n",
    "| 3 | Final presentation preparation | All menbers | 2h\n",
    "| 3 | Final report preparation and completion (need to update later) | All members | 10h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf381c1e-739c-413b-a3b2-8e2320852304",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "\n",
    "Flight delays incur economic losses and customer inconvenience in the airline industry. Our goal is to provide a scalable machine learning solution based on historical data to forecast flight delays to avoid excessive costs for our airline company. In this first phase of our project, we analyze the dataset sourced from the U.S. Department of Transportation and the transformations needed on the dataset to train our predictive models. We also describe the machine learning models we will pursue to predict flight delays, including logistic regression, random forest, gradient boosting, and neural networks. We also define precision, recall and F1 score as the metrics of success, aiming to minimize false positives (discussed extensively in the algorithm section), to capture the viability of the models. To accomplish the goal of devising a scalable machine learning solution, we will take advantage of the highly optimized big data framework, i.e. PySpark, for data processing and modeling. Lastly, we explain the timeline and steps for project implementation, expecting the project to be completed mid-December."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc1640c-49f8-4fa2-99c8-383061f0b375",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Description\n",
    "Our scalable machine learing solution will utilize four distinct datasets. These datasets pertain to both airlines and weather data, encompassing information spanning different timeframes, including three months, six months, one year, as well as a comprehensive dataset covering the years between 2015 and 2019. Additionally, there is a supplementary dataset called \"Weather Stations,\" which furnishes location details for various weather stations. The central dataset, critical to our project, is the OTPW dataset, which amalgamates data from both Weather and Airlines, forming the backbone of our analysis.\n",
    "\n",
    "To initiate our exploration of the available data, we commenced with an examination of the \"OTPW_3M_2015.csv\" file, which captures data for the period from January 1, 2015, to March 31, 2015. This specific dataset comprises 1202 rows and an extensive 216 columns. Within this vast array, we identified 88 columns that exhibited limited utility for our analysis, as they contained data in fewer than five records.\n",
    "\n",
    "Upon delving into the dataset, we made several noteworthy observations. First and foremost, our analysis revealed that, among the states, California, Texas, Florida, Illinois, and New York experienced the highest frequency of delays in departure flights (as illustrated in Figure 1). These states, in particular, showcased a substantial prevalence of such delays.\n",
    "\n",
    "Further investigation into the causes of departure flight delays unveiled that carrier delays and late aircraft delays were the primary culprits. Notably, the National Aviation System (NAS) emerged as the next significant source of delays. Weather-related delays occupied the fourth position in the hierarchy of delay causes, while security-related delays appeared to be less significant (as depicted in Figure 2). This indicates that, for the dataset under consideration, carrier delays and late aircraft delays predominantly contributed to the delays experienced in departure flights.\n",
    "\n",
    "However, it is interesting to note that while Florida featured among the top five states with the highest flight delays, the majority of its delays were not attributable to weather-related issues. In contrast, the state of Georgia emerged among the top five states with a significant prevalence of weather-related delays. This observation suggests that weather conditions played a more substantial role in the delays experienced by flights in Georgia compared to Florida (as illustrated in Figure 3). This figure effectively highlights the states where weather-related factors held greater significance in contributing to flight delays.\n",
    "\n",
    "In phase 2 of this project, we will conduct a comprehensive exploratory data analysis, encompassing data cleaning and formatting to ensure a thorough understanding of the datasets for our objective.\n",
    "\n",
    "#### [Reference Notebook](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/1517543026970395)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88aa2b6e-7720-4c62-a6ca-6256f668190c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Machine Learning Algorithms and Metrics\n",
    "\n",
    "Our primary interest is to forecast whether a flight will be delayed, given the information available in the datasets described in the previous section, and if so, by how long. After some basic EDA, we identify some features in the dataset about historically, the Airline flights' delayed time, in numerical form. However, as we plan on deplying the state-of-the-art algorithms such as neural networks which are available in the realm of classification, we decide to frame the prediction task into a classification task to determine flight delay, by categorying the delayed time into several degrees with 0 being no delay, and other classes being the severity of the delay. We will employ a number of different ML algorithms available in PySpark's MLlib package.\n",
    "\n",
    "#### Logistic regression\n",
    "Linear/logistic regression will serve as our starting algorithm and the baseline, to test viability and be used as a comparison for more advanced algorithms. In logistic regression, we want to model the relationship between our features and flight delay by fitting a linear equation to the data. We can use the readily available MLlib package in PySpark to implement this model. Depending on whether our target will be binary or multi-class, we will use either binary cross-entropy or categorical cross-entropy loss functions for the model optimization. $$J(\\theta) = -[y \\log(h_\\theta(x)) + (1 - y) \\log(1 - h_\\theta(x))] $$ where y is the ground truth and h(x) is the prediction.\n",
    "\n",
    "#### Random Forest Classifier \n",
    "To improve our pipeline over the baseline, we will use Random Forest Classifier, an ensemble learning method combining multiple decision tree classifiers to achieve a more accurate prediction, capable of handling non linear data. We will use the MLlib’s RandomForestRegressor in PySpark. \n",
    "\n",
    "#### Gradient Boosting Classifier\n",
    "Simular to the method above, Gradient boosting is an ensemble method building a sequence of decision trees in attempts to minimize the residuals of the prior tree. If MLlib does not have GBC available, we will use xgboost library’s GradientBoostingClassifier.\n",
    "\n",
    "#### Neural Network\n",
    "Neural networks involve using a combination of layers and nodes with associated weights and activation functions to capture even more complex relationships inthe data. To implement such an algorithm, we will use MLlib's multilayer preceptron algorithm. The algorithm will use gradient descent to update the weights and biases of the model in a way to minimize the cross-entropy loss. \n",
    "\n",
    "#### Metrics\n",
    "In the case where we choose to use a categorical result (classification problem), being as we want to predict delay, slight delay, significant delay, we can employ measurements of Recall and Precision. Recall is used to measure the true positive rate determined by the model, designated as $$\\text{Recall} = \\frac{TP}{TP + FN} \\$$, while Precision is used to measure the number of Positive identifications being actually correct, designated as $$\\\\text{Precision} = \\frac{TP}{TP + FP} \\$$. We can then use the F1 score, defined as $$\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\$$ to help us strike a balance between precision and recall, and further summarizing the model’s accuracy. In the case of flight delays, it may be that we want to avoid false positives (Predicted delay even though there wasn’t) over false negatives (It is better that a passenger comes early for a false negative than coming late to a flight due to a false positive), and we can use the F1 score to ensure recall dosen’t dip below an acceptable threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be17447e-a396-42c0-8d7d-3abbdcf71e84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Machine Learning Pipelines\n",
    "\n",
    "Our Airline flight delay prediction project was launched on October 25, 2023, and will be completed by December 17, 2023. The pipeline consists of 3 phases. The following are the key steps and timing involved in each stage.\n",
    "\n",
    "- **Phase 1:** We will have our tech stack set up, including Databricks and Blob Storage access, for team collaboration. Next, we will perform a simple EDA to understand the data we have in hand for modeling. A project plan will be finalized at this stage with an elaboration on the task assignment, data processing, algorithms and metrics of success. Phase 1 will end on November 6, 2023.\n",
    "\n",
    "- **Phase 2:** A comprehensive EDA will be implemented on raw data and supplementary data. Then, we will perform data proprocessing, such as data imputation, feature selection, data transformation, based on techniques such as univariate feature selection, recursive feature elimination and random forest feature importance, we will filter out critical features and consider using PCA. Then the data set is divided into a training set and a test set for training. Details in training and evaluation can be referred to \"Machine algorithms and metrics\". These works will be conducted on Microsoft Databricks and Pyspark will be used to process data. At the end of this phase, we will have a baseline pipeline using the simple logistics regression to classify flight delay. The output will be summarized in a code notebook and presentation. This phase will end on December 4, 2023.\n",
    "- **Phase 3:** This phase focuses on more complex model exploration, experiments, evaluation and fine-tuning. We will implement more complicated models to improve our evaluation metrics. We will also experiment with new features and perform feature engineering on all data as well as explore novel research directions. The results of this phase will be summarized in the code and final presentation. The third phase is expected to last for half a month.\n",
    "\n",
    "**Below shows the timeline diagram for machine learning pipeline construction and the flow chart for the project implementation process.**\n",
    "\n",
    "<img src='https://github.com/RaphaelYangWJ/w261/blob/main/timeline.png?raw=true'>\n",
    "<img src='https://github.com/RaphaelYangWJ/w261/blob/main/pro_flow_chart.png?raw=true'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea6320f0-6b42-46b6-9dcf-a6467e57cd1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "With phase 1 of this project, we have successfully framed our machine learning task, along with finalizing plans for data processing, feature engineering, modeling and evaluation. We have distributed equally tasks to the team members as well as the objective/goal for each task. We are ready to enter phase 2 for a more thorough EDA of our datasets and creating a baseline pipeline. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "FP Phase 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
